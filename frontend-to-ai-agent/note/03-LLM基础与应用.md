# 03 - LLM åŸºç¡€ä¸åº”ç”¨

> **å‰ç½®æ¡ä»¶**ï¼šå®Œæˆ Python åŸºç¡€å’Œ FastAPI å­¦ä¹   
> **å­¦ä¹ æ—¶é•¿**ï¼š2-3 å‘¨  
> **å­¦ä¹ ç›®æ ‡**ï¼šæŒæ¡å¤§è¯­è¨€æ¨¡å‹çš„åŸºæœ¬åŸç†å’Œ OpenAI API ä½¿ç”¨

---

## ğŸ¯ å­¦ä¹ ç›®æ ‡

- ç†è§£å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åŸºæœ¬åŸç†
- æŒæ¡ OpenAI API çš„ä½¿ç”¨
- å­¦ä¹  Prompt Engineeringï¼ˆæç¤ºè¯å·¥ç¨‹ï¼‰
- ç†è§£ Tokenã€Embeddingsã€Function Calling
- èƒ½å¤Ÿæ„å»ºåŸºç¡€çš„ AI åº”ç”¨

---

## ğŸ“š å­¦ä¹ å†…å®¹

### 1. LLM åŸºç¡€æ¦‚å¿µ

#### 1.1 ä»€ä¹ˆæ˜¯å¤§è¯­è¨€æ¨¡å‹ï¼Ÿ

**æ ¸å¿ƒæ¦‚å¿µ**ï¼š
- **Transformer æ¶æ„**ï¼šåŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„ç¥ç»ç½‘ç»œ
- **é¢„è®­ç»ƒ**ï¼šåœ¨æµ·é‡æ–‡æœ¬æ•°æ®ä¸Šå­¦ä¹ è¯­è¨€æ¨¡å¼
- **ç”Ÿæˆå¼ AI**ï¼šæ ¹æ®è¾“å…¥ç”Ÿæˆæ–‡æœ¬è¾“å‡º
- **ä¸Šä¸‹æ–‡çª—å£**ï¼šæ¨¡å‹èƒ½å¤Ÿ"è®°ä½"çš„æ–‡æœ¬é•¿åº¦

**ä¸»æµ LLM å¯¹æ¯”**ï¼š

| æ¨¡å‹ | ä¸Šä¸‹æ–‡çª—å£ | ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |
|------|-----------|------|---------|
| GPT-4 | 8K / 32K / 128K | æœ€å¼ºèƒ½åŠ› | å¤æ‚æ¨ç†ã€ä»£ç ç”Ÿæˆ |
| GPT-3.5-turbo | 16K | æ€§ä»·æ¯”é«˜ | èŠå¤©ã€ç®€å•ä»»åŠ¡ |
| Claude 3 | 200K | è¶…é•¿ä¸Šä¸‹æ–‡ | æ–‡æ¡£åˆ†æ |
| Gemini | 1M | å¤šæ¨¡æ€ | å›¾åƒ+æ–‡æœ¬ |

---

#### 1.2 Token æ¦‚å¿µ

**ä»€ä¹ˆæ˜¯ Tokenï¼Ÿ**
- Token æ˜¯ LLM å¤„ç†æ–‡æœ¬çš„åŸºæœ¬å•ä½
- 1 ä¸ª Token â‰ˆ 0.75 ä¸ªè‹±æ–‡å•è¯
- 1 ä¸ªä¸­æ–‡å­—ç¬¦ â‰ˆ 2-3 ä¸ª Token

**ç¤ºä¾‹**ï¼š
```
"Hello, World!" â†’ 4 tokens: ["Hello", ",", " World", "!"]
"ä½ å¥½ä¸–ç•Œ" â†’ 6-8 tokensï¼ˆä¸­æ–‡ token åŒ–è¾ƒå¤æ‚ï¼‰
```

**æˆæœ¬è®¡ç®—**ï¼š
```python
# GPT-4 ä»·æ ¼ï¼ˆ2024å¹´ï¼‰
# Input: $0.03 / 1K tokens
# Output: $0.06 / 1K tokens

# ç¤ºä¾‹ï¼š1000 å­—æ–‡ç« åˆ†æ
input_tokens = 1500  # è¾“å…¥ token
output_tokens = 500  # è¾“å‡º token

cost = (input_tokens / 1000 * 0.03) + (output_tokens / 1000 * 0.06)
print(f"æˆæœ¬ï¼š${cost:.4f}")  # $0.0750
```

---

### 2. OpenAI API ä½¿ç”¨

#### 2.1 å®‰è£…ä¸é…ç½®

```bash
pip install openai
```

```python
import os
from openai import OpenAI

# è®¾ç½® API Key
os.environ["OPENAI_API_KEY"] = "sk-your-api-key"
client = OpenAI()
```

---

#### 2.2 åŸºç¡€å¯¹è¯

```python
def chat_with_gpt(user_message):
    response = client.chat.completions.create(
        model="gpt-4",  # æˆ– "gpt-3.5-turbo"
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹ã€‚"},
            {"role": "user", "content": user_message}
        ],
        temperature=0.7,  # æ§åˆ¶éšæœºæ€§ (0-2)
        max_tokens=500,   # æœ€å¤§è¾“å‡º token æ•°
    )
    return response.choices[0].message.content

# ä½¿ç”¨
result = chat_with_gpt("è§£é‡Šä»€ä¹ˆæ˜¯ Python è£…é¥°å™¨")
print(result)
```

---

#### 2.3 å¤šè½®å¯¹è¯ï¼ˆå¸¦è®°å¿†ï¼‰

```python
class ChatBot:
    def __init__(self, system_prompt="ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹ã€‚"):
        self.messages = [
            {"role": "system", "content": system_prompt}
        ]
    
    def chat(self, user_message):
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        self.messages.append({"role": "user", "content": user_message})
        
        # è°ƒç”¨ API
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=self.messages
        )
        
        # æ·»åŠ åŠ©æ‰‹å›å¤
        assistant_message = response.choices[0].message.content
        self.messages.append({"role": "assistant", "content": assistant_message})
        
        return assistant_message
    
    def clear_history(self):
        self.messages = self.messages[:1]  # ä¿ç•™ç³»ç»Ÿæç¤º

# ä½¿ç”¨
bot = ChatBot()
print(bot.chat("æˆ‘çš„åå­—æ˜¯ Alice"))
print(bot.chat("æˆ‘å«ä»€ä¹ˆåå­—ï¼Ÿ"))  # AI ä¼šè®°ä½ä¹‹å‰çš„å¯¹è¯
```

---

### 3. Prompt Engineeringï¼ˆæç¤ºè¯å·¥ç¨‹ï¼‰

#### 3.1 åŸºæœ¬åŸåˆ™

**1. æ¸…æ™°æ˜ç¡®**
```python
# âŒ ä¸å¥½çš„æç¤º
"å†™ç‚¹å…³äº AI çš„ä¸œè¥¿"

# âœ… å¥½çš„æç¤º
"å†™ä¸€ç¯‡ 500 å­—çš„æ–‡ç« ï¼Œä»‹ç» AI åœ¨åŒ»ç–—é¢†åŸŸçš„ 3 ä¸ªåº”ç”¨æ¡ˆä¾‹ï¼ŒåŒ…æ‹¬è¯Šæ–­ã€è¯ç‰©ç ”å‘å’Œä¸ªæ€§åŒ–æ²»ç–—ã€‚"
```

**2. æä¾›ä¸Šä¸‹æ–‡**
```python
# âœ… æä¾›è§’è‰²å’ŒèƒŒæ™¯
prompt = """
ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„ Python æ•™å¸ˆã€‚
å­¦ç”Ÿåˆšå­¦å®ŒåŸºç¡€è¯­æ³•ï¼Œæ­£åœ¨å­¦ä¹ é¢å‘å¯¹è±¡ç¼–ç¨‹ã€‚
è¯·ç”¨ç®€å•çš„è¯­è¨€è§£é‡Šä»€ä¹ˆæ˜¯ç±»å’Œå¯¹è±¡ï¼Œå¹¶ç»™å‡ºä¸€ä¸ªç”Ÿæ´»ä¸­çš„ä¾‹å­ã€‚
"""
```

**3. ä½¿ç”¨ç¤ºä¾‹ï¼ˆFew-shot Learningï¼‰**
```python
prompt = """
å°†ä»¥ä¸‹æ–‡æœ¬åˆ†ç±»ä¸ºï¼šæ­£é¢ã€è´Ÿé¢ã€ä¸­æ€§

ç¤ºä¾‹ï¼š
æ–‡æœ¬ï¼š"è¿™ä¸ªäº§å“å¤ªæ£’äº†ï¼" â†’ æ­£é¢
æ–‡æœ¬ï¼š"è´¨é‡ä¸€èˆ¬ï¼Œä»·æ ¼åé«˜ã€‚" â†’ è´Ÿé¢
æ–‡æœ¬ï¼š"è¿™æ˜¯ä¸€ä¸ªè“è‰²çš„æ¯å­ã€‚" â†’ ä¸­æ€§

æ–‡æœ¬ï¼š"æœåŠ¡æ€åº¦å¾ˆå¥½ï¼Œä½†ç­‰å¾…æ—¶é—´æœ‰ç‚¹é•¿ã€‚" â†’ ?
"""
```

---

#### 3.2 é«˜çº§æŠ€å·§

**Chain of Thoughtï¼ˆæ€ç»´é“¾ï¼‰**
```python
prompt = """
é—®é¢˜ï¼šä¸€ä¸ªç­çº§æœ‰ 30 åå­¦ç”Ÿï¼Œå…¶ä¸­ 60% æ˜¯å¥³ç”Ÿã€‚å¥³ç”Ÿä¸­æœ‰ 40% æˆ´çœ¼é•œã€‚è¯·é—®æœ‰å¤šå°‘å¥³ç”Ÿæˆ´çœ¼é•œï¼Ÿ

è¯·ä¸€æ­¥æ­¥æ€è€ƒï¼š
1. é¦–å…ˆè®¡ç®—å¥³ç”Ÿäººæ•°
2. ç„¶åè®¡ç®—æˆ´çœ¼é•œçš„å¥³ç”Ÿäººæ•°
3. ç»™å‡ºæœ€ç»ˆç­”æ¡ˆ
"""
```

**è§’è‰²æ‰®æ¼”**
```python
system_prompt = """
ä½ æ˜¯ä¸€ä½èµ„æ·±çš„å‰ç«¯é¢è¯•å®˜ï¼Œæœ‰ 10 å¹´ç»éªŒã€‚
ä½ éœ€è¦ï¼š
- æå‡ºæœ‰æ·±åº¦çš„æŠ€æœ¯é—®é¢˜
- æ ¹æ®å›ç­”ç»™å‡ºå»ºè®¾æ€§åé¦ˆ
- è¯„ä¼°å€™é€‰äººçš„æŠ€æœ¯æ°´å¹³
"""
```

---

### 4. æµå¼å“åº”ï¼ˆStreamingï¼‰

```python
def stream_chat(user_message):
    stream = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": user_message}],
        stream=True  # å¯ç”¨æµå¼è¾“å‡º
    )
    
    for chunk in stream:
        if chunk.choices[0].delta.content is not None:
            content = chunk.choices[0].delta.content
            print(content, end="", flush=True)

stream_chat("å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—")
```

**åœ¨ FastAPI ä¸­ä½¿ç”¨æµå¼å“åº”**ï¼š
```python
from fastapi import FastAPI
from fastapi.responses import StreamingResponse

app = FastAPI()

@app.get("/stream-chat")
async def stream_chat(message: str):
    def generate():
        stream = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": message}],
            stream=True
        )
        for chunk in stream:
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content
    
    return StreamingResponse(generate(), media_type="text/plain")
```

---

### 5. Function Callingï¼ˆå‡½æ•°è°ƒç”¨ï¼‰

#### 5.1 åŸºæœ¬æ¦‚å¿µ

Function Calling è®© LLM èƒ½å¤Ÿï¼š
- è¯†åˆ«ä½•æ—¶éœ€è¦è°ƒç”¨å¤–éƒ¨å‡½æ•°
- ç”Ÿæˆæ­£ç¡®çš„å‡½æ•°å‚æ•°
- å°†ç»“æœåé¦ˆç»™ç”¨æˆ·

**ä½¿ç”¨åœºæ™¯**ï¼š
- æŸ¥è¯¢æ•°æ®åº“
- è°ƒç”¨å¤©æ°” API
- æ‰§è¡Œè®¡ç®—
- ä¸å¤–éƒ¨ç³»ç»Ÿäº¤äº’

---

#### 5.2 å®æˆ˜ç¤ºä¾‹

```python
import json

# å®šä¹‰å¯ç”¨çš„å‡½æ•°
def get_weather(location, unit="celsius"):
    """è·å–å¤©æ°”ä¿¡æ¯ï¼ˆæ¨¡æ‹Ÿï¼‰"""
    return {
        "location": location,
        "temperature": 22,
        "unit": unit,
        "condition": "æ™´å¤©"
    }

# å®šä¹‰å‡½æ•°æè¿°
functions = [
    {
        "name": "get_weather",
        "description": "è·å–æŒ‡å®šåœ°ç‚¹çš„å¤©æ°”ä¿¡æ¯",
        "parameters": {
            "type": "object",
            "properties": {
                "location": {
                    "type": "string",
                    "description": "åŸå¸‚åç§°ï¼Œä¾‹å¦‚ï¼šåŒ—äº¬"
                },
                "unit": {
                    "type": "string",
                    "enum": ["celsius", "fahrenheit"],
                    "description": "æ¸©åº¦å•ä½"
                }
            },
            "required": ["location"]
        }
    }
]

def chat_with_function_calling(user_message):
    # ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼šè®© GPT å†³å®šæ˜¯å¦éœ€è¦è°ƒç”¨å‡½æ•°
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": user_message}],
        functions=functions,
        function_call="auto"
    )
    
    message = response.choices[0].message
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦è°ƒç”¨å‡½æ•°
    if message.function_call:
        function_name = message.function_call.name
        function_args = json.loads(message.function_call.arguments)
        
        # è°ƒç”¨å®é™…å‡½æ•°
        if function_name == "get_weather":
            function_response = get_weather(**function_args)
        
        # ç¬¬äºŒæ¬¡è°ƒç”¨ï¼šå°†å‡½æ•°ç»“æœè¿”å›ç»™ GPT
        second_response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "user", "content": user_message},
                message,
                {
                    "role": "function",
                    "name": function_name,
                    "content": json.dumps(function_response)
                }
            ]
        )
        return second_response.choices[0].message.content
    
    return message.content

# ä½¿ç”¨
result = chat_with_function_calling("åŒ—äº¬ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ")
print(result)  # GPT ä¼šè°ƒç”¨ get_weather å‡½æ•°å¹¶ç”Ÿæˆè‡ªç„¶è¯­è¨€å›å¤
```

---

### 6. Embeddingsï¼ˆå‘é‡åµŒå…¥ï¼‰

#### 6.1 ä»€ä¹ˆæ˜¯ Embeddingsï¼Ÿ

- å°†æ–‡æœ¬è½¬æ¢ä¸ºé«˜ç»´å‘é‡ï¼ˆæ•°å­—æ•°ç»„ï¼‰
- è¯­ä¹‰ç›¸ä¼¼çš„æ–‡æœ¬æœ‰ç›¸ä¼¼çš„å‘é‡
- ç”¨äºè¯­ä¹‰æœç´¢ã€æ¨èç³»ç»Ÿã€èšç±»åˆ†æ

```python
def get_embedding(text):
    response = client.embeddings.create(
        model="text-embedding-ada-002",
        input=text
    )
    return response.data[0].embedding

# è·å–æ–‡æœ¬çš„å‘é‡è¡¨ç¤º
text = "Python æ˜¯ä¸€é—¨ç¼–ç¨‹è¯­è¨€"
embedding = get_embedding(text)
print(f"å‘é‡ç»´åº¦ï¼š{len(embedding)}")  # 1536
print(f"å‰ 5 ä¸ªå€¼ï¼š{embedding[:5]}")
```

---

#### 6.2 è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—

```python
import numpy as np

def cosine_similarity(vec1, vec2):
    """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))

# æ¯”è¾ƒä¸¤ä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦
text1 = "Python æ˜¯ä¸€é—¨ç¼–ç¨‹è¯­è¨€"
text2 = "JavaScript æ˜¯ä¸€ç§ç¼–ç¨‹è¯­è¨€"
text3 = "ä»Šå¤©å¤©æ°”å¾ˆå¥½"

emb1 = get_embedding(text1)
emb2 = get_embedding(text2)
emb3 = get_embedding(text3)

print(f"æ–‡æœ¬1 vs æ–‡æœ¬2ï¼š{cosine_similarity(emb1, emb2):.4f}")  # é«˜ç›¸ä¼¼åº¦
print(f"æ–‡æœ¬1 vs æ–‡æœ¬3ï¼š{cosine_similarity(emb1, emb3):.4f}")  # ä½ç›¸ä¼¼åº¦
```

---

### 7. å®Œæ•´é¡¹ç›®ç¤ºä¾‹

#### é¡¹ç›®ï¼šAI èŠå¤©æœºå™¨äººï¼ˆå¸¦æµå¼å“åº”ï¼‰

```python
# main.py
from fastapi import FastAPI, WebSocket
from fastapi.middleware.cors import CORSMiddleware
import json

app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class ChatSession:
    def __init__(self):
        self.messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„ AI åŠ©æ‰‹ã€‚"}
        ]
    
    def add_message(self, role, content):
        self.messages.append({"role": role, "content": content})
    
    def get_response_stream(self):
        stream = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=self.messages,
            stream=True
        )
        
        full_response = ""
        for chunk in stream:
            if chunk.choices[0].delta.content:
                content = chunk.choices[0].delta.content
                full_response += content
                yield content
        
        self.add_message("assistant", full_response)

sessions = {}

@app.websocket("/ws/chat/{session_id}")
async def websocket_chat(websocket: WebSocket, session_id: str):
    await websocket.accept()
    
    # åˆ›å»ºæˆ–è·å–ä¼šè¯
    if session_id not in sessions:
        sessions[session_id] = ChatSession()
    
    session = sessions[session_id]
    
    try:
        while True:
            # æ¥æ”¶ç”¨æˆ·æ¶ˆæ¯
            user_message = await websocket.receive_text()
            session.add_message("user", user_message)
            
            # æµå¼å‘é€ AI å›å¤
            for chunk in session.get_response_stream():
                await websocket.send_text(json.dumps({
                    "type": "chunk",
                    "content": chunk
                }))
            
            # å‘é€å®Œæˆä¿¡å·
            await websocket.send_text(json.dumps({
                "type": "done"
            }))
    except Exception as e:
        print(f"WebSocket error: {e}")
```

**å‰ç«¯ç¤ºä¾‹ï¼ˆReactï¼‰**ï¼š
```javascript
import { useState, useEffect } from 'react';

function ChatApp() {
    const [messages, setMessages] = useState([]);
    const [input, setInput] = useState('');
    const [ws, setWs] = useState(null);
    const [currentResponse, setCurrentResponse] = useState('');

    useEffect(() => {
        const websocket = new WebSocket('ws://localhost:8000/ws/chat/user123');
        
        websocket.onmessage = (event) => {
            const data = JSON.parse(event.data);
            
            if (data.type === 'chunk') {
                setCurrentResponse(prev => prev + data.content);
            } else if (data.type === 'done') {
                setMessages(prev => [...prev, {
                    role: 'assistant',
                    content: currentResponse
                }]);
                setCurrentResponse('');
            }
        };
        
        setWs(websocket);
        return () => websocket.close();
    }, []);

    const sendMessage = () => {
        if (ws && input.trim()) {
            setMessages(prev => [...prev, { role: 'user', content: input }]);
            ws.send(input);
            setInput('');
        }
    };

    return (
        <div>
            <div className="messages">
                {messages.map((msg, i) => (
                    <div key={i} className={msg.role}>
                        {msg.content}
                    </div>
                ))}
                {currentResponse && (
                    <div className="assistant">{currentResponse}</div>
                )}
            </div>
            <input 
                value={input}
                onChange={(e) => setInput(e.target.value)}
                onKeyPress={(e) => e.key === 'Enter' && sendMessage()}
            />
            <button onClick={sendMessage}>å‘é€</button>
        </div>
    );
}
```

---

## ğŸ¯ å®æˆ˜ç»ƒä¹ 

### ç»ƒä¹  1ï¼šæ™ºèƒ½æ–‡æœ¬æ‘˜è¦
```python
def summarize_text(long_text, max_words=100):
    prompt = f"""
    è¯·å°†ä»¥ä¸‹æ–‡æœ¬æ€»ç»“ä¸ºä¸è¶…è¿‡ {max_words} å­—çš„æ‘˜è¦ï¼š
    
    {long_text}
    """
    
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}]
    )
    return response.choices[0].message.content
```

### ç»ƒä¹  2ï¼šä»£ç è§£é‡Šå™¨
```python
def explain_code(code, language="python"):
    prompt = f"""
    è¯·è§£é‡Šä»¥ä¸‹ {language} ä»£ç çš„åŠŸèƒ½ï¼Œç”¨ç®€å•çš„è¯­è¨€è¯´æ˜ï¼š
    
    ```{language}
    {code}
    ```
    """
    
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}]
    )
    return response.choices[0].message.content
```

### ç»ƒä¹  3ï¼šæ™ºèƒ½æœç´¢ï¼ˆè¯­ä¹‰æœç´¢ï¼‰
```python
class SemanticSearch:
    def __init__(self):
        self.documents = []
        self.embeddings = []
    
    def add_document(self, text):
        self.documents.append(text)
        self.embeddings.append(get_embedding(text))
    
    def search(self, query, top_k=3):
        query_embedding = get_embedding(query)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = [
            cosine_similarity(query_embedding, doc_emb)
            for doc_emb in self.embeddings
        ]
        
        # è¿”å›æœ€ç›¸ä¼¼çš„æ–‡æ¡£
        top_indices = np.argsort(similarities)[-top_k:][::-1]
        return [(self.documents[i], similarities[i]) for i in top_indices]

# ä½¿ç”¨
search_engine = SemanticSearch()
search_engine.add_document("Python æ˜¯ä¸€é—¨ç¼–ç¨‹è¯­è¨€")
search_engine.add_document("æœºå™¨å­¦ä¹ æ˜¯ AI çš„å­é¢†åŸŸ")
search_engine.add_document("ä»Šå¤©å¤©æ°”å¾ˆå¥½")

results = search_engine.search("ä»€ä¹ˆæ˜¯ç¼–ç¨‹ï¼Ÿ")
for doc, score in results:
    print(f"{score:.4f}: {doc}")
```

---

## ğŸ“– æ¨èèµ„æº

### å®˜æ–¹æ–‡æ¡£
- **OpenAI API æ–‡æ¡£**ï¼šhttps://platform.openai.com/docs
- **Prompt Engineering æŒ‡å—**ï¼šhttps://platform.openai.com/docs/guides/prompt-engineering

### å­¦ä¹ èµ„æº
- **OpenAI Cookbook**ï¼šhttps://github.com/openai/openai-cookbook
- **Learn Prompting**ï¼šhttps://learnprompting.org/
- **Prompt Engineering Guide**ï¼šhttps://www.promptingguide.ai/

### å®æˆ˜é¡¹ç›®
- æ„å»ºæ™ºèƒ½å®¢æœ
- å¼€å‘ä»£ç åŠ©æ‰‹
- åˆ›å»ºå†…å®¹ç”Ÿæˆå™¨

---

## âœ… å­¦ä¹ æ£€æŸ¥æ¸…å•

- [ ] ç†è§£ LLM å’Œ Token çš„åŸºæœ¬æ¦‚å¿µ
- [ ] èƒ½å¤Ÿä½¿ç”¨ OpenAI API è¿›è¡Œå¯¹è¯
- [ ] æŒæ¡ Prompt Engineering æŠ€å·§
- [ ] å®ç°æµå¼å“åº”
- [ ] ç†è§£å¹¶ä½¿ç”¨ Function Calling
- [ ] æŒæ¡ Embeddings å’Œè¯­ä¹‰æœç´¢
- [ ] å®Œæˆè‡³å°‘ 2 ä¸ªå®Œæ•´çš„ AI åº”ç”¨é¡¹ç›®

---

**ä¸‹ä¸€æ­¥**ï¼šå­¦ä¹  [04-LangChainæ¡†æ¶](./04-LangChainæ¡†æ¶.md)
