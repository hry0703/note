# 04 - LangChain æ¡†æ¶

> **å‰ç½®æ¡ä»¶**ï¼šå®Œæˆ Python åŸºç¡€ã€FastAPI å’Œ LLM åŸºç¡€å­¦ä¹   
> **å­¦ä¹ æ—¶é•¿**ï¼š3-4 å‘¨  
> **å­¦ä¹ ç›®æ ‡**ï¼šæŒæ¡ LangChain æ¡†æ¶ï¼Œèƒ½å¤Ÿæ„å»ºå¤æ‚çš„ AI åº”ç”¨

---

## ğŸ¯ ä¸ºä»€ä¹ˆå­¦ä¹  LangChainï¼Ÿ

### LangChain è§£å†³çš„é—®é¢˜

**ä¸ä½¿ç”¨ LangChain**ï¼š
```python
# å¤æ‚ã€é‡å¤çš„ä»£ç 
response = openai.chat.completions.create(...)
# éœ€è¦æ‰‹åŠ¨ç®¡ç†å¯¹è¯å†å²
# éœ€è¦æ‰‹åŠ¨å¤„ç†æ–‡æ¡£åŠ è½½å’Œåˆ†å—
# éœ€è¦æ‰‹åŠ¨å®ç°å‘é‡æ£€ç´¢
# éœ€è¦æ‰‹åŠ¨ä¸²è”å¤šä¸ª LLM è°ƒç”¨
```

**ä½¿ç”¨ LangChain**ï¼š
```python
# ç®€æ´ã€å¯å¤ç”¨çš„ä»£ç 
from langchain.chains import LLMChain
chain = LLMChain(llm=llm, prompt=prompt)
result = chain.run("ç”¨æˆ·è¾“å…¥")
```

**LangChain çš„æ ¸å¿ƒä»·å€¼**ï¼š
- ğŸ”— **é“¾å¼è°ƒç”¨**ï¼šè½»æ¾ç»„åˆå¤šä¸ªæ­¥éª¤
- ğŸ’¾ **è®°å¿†ç®¡ç†**ï¼šè‡ªåŠ¨å¤„ç†å¯¹è¯å†å²
- ğŸ“š **æ–‡æ¡£å¤„ç†**ï¼šå†…ç½®æ–‡æ¡£åŠ è½½å’Œåˆ†å—
- ğŸ” **å‘é‡æ£€ç´¢**ï¼šé›†æˆå‘é‡æ•°æ®åº“
- ğŸ¤– **Agent æ¡†æ¶**ï¼šæ„å»ºè‡ªä¸»å†³ç­–çš„ AI Agent

---

## ğŸ“š å­¦ä¹ å†…å®¹

### 1. LangChain æ ¸å¿ƒæ¦‚å¿µ

#### 1.1 å®‰è£…

```bash
pip install langchain langchain-openai langchain-community
pip install chromadb  # å‘é‡æ•°æ®åº“
pip install pypdf  # PDF æ–‡æ¡£å¤„ç†
pip install faiss-cpu  # å‘é‡æ£€ç´¢ï¼ˆå¯é€‰ï¼‰
```

---

#### 1.2 æ ¸å¿ƒç»„ä»¶

```
LangChain æ ¸å¿ƒæ¶æ„ï¼š

Modelsï¼ˆæ¨¡å‹ï¼‰
   â†“
Promptsï¼ˆæç¤ºè¯æ¨¡æ¿ï¼‰
   â†“
Chainsï¼ˆé“¾å¼è°ƒç”¨ï¼‰
   â†“
Memoryï¼ˆè®°å¿†ï¼‰
   â†“
Agentsï¼ˆæ™ºèƒ½ä»£ç†ï¼‰
```

---

### 2. Modelsï¼ˆæ¨¡å‹å°è£…ï¼‰

#### 2.1 LLM å’Œ Chat Models

```python
from langchain_openai import ChatOpenAI, OpenAI

# Chat Modelsï¼ˆç”¨äºå¯¹è¯ï¼‰
chat_model = ChatOpenAI(
    model="gpt-3.5-turbo",
    temperature=0.7,
    max_tokens=500
)

# LLMï¼ˆç”¨äºæ–‡æœ¬ç”Ÿæˆï¼‰
llm = OpenAI(
    model="gpt-3.5-turbo-instruct",
    temperature=0.7
)

# ç›´æ¥è°ƒç”¨
response = chat_model.invoke("Hello!")
print(response.content)
```

---

#### 2.2 æ¶ˆæ¯ç±»å‹

```python
from langchain.schema import HumanMessage, SystemMessage, AIMessage

messages = [
    SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹ã€‚"),
    HumanMessage(content="Python æ˜¯ä»€ä¹ˆï¼Ÿ"),
]

response = chat_model.invoke(messages)
print(response.content)
```

---

### 3. Promptsï¼ˆæç¤ºè¯æ¨¡æ¿ï¼‰

#### 3.1 PromptTemplate

```python
from langchain.prompts import PromptTemplate

# åˆ›å»ºæ¨¡æ¿
template = """
ä½ æ˜¯ä¸€ä¸ª{role}ã€‚
è¯·å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š{question}
"""

prompt = PromptTemplate(
    input_variables=["role", "question"],
    template=template
)

# æ ¼å¼åŒ–
formatted_prompt = prompt.format(
    role="Python ä¸“å®¶",
    question="ä»€ä¹ˆæ˜¯è£…é¥°å™¨ï¼Ÿ"
)
print(formatted_prompt)
```

---

#### 3.2 ChatPromptTemplate

```python
from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate

# åˆ›å»ºèŠå¤©æ¨¡æ¿
system_template = "ä½ æ˜¯ä¸€ä¸ª{role}ã€‚"
human_template = "{question}"

chat_prompt = ChatPromptTemplate.from_messages([
    SystemMessagePromptTemplate.from_template(system_template),
    HumanMessagePromptTemplate.from_template(human_template)
])

# æ ¼å¼åŒ–
messages = chat_prompt.format_messages(
    role="Python ä¸“å®¶",
    question="ä»€ä¹ˆæ˜¯è£…é¥°å™¨ï¼Ÿ"
)

response = chat_model.invoke(messages)
print(response.content)
```

---

#### 3.3 Few-shot Prompts

```python
from langchain.prompts import FewShotPromptTemplate

# ç¤ºä¾‹æ•°æ®
examples = [
    {"input": "å¼€å¿ƒ", "output": "ğŸ˜Š"},
    {"input": "æ‚²ä¼¤", "output": "ğŸ˜¢"},
    {"input": "æ„¤æ€’", "output": "ğŸ˜ "}
]

# ç¤ºä¾‹æ¨¡æ¿
example_template = """
è¾“å…¥ï¼š{input}
è¾“å‡ºï¼š{output}
"""

example_prompt = PromptTemplate(
    input_variables=["input", "output"],
    template=example_template
)

# Few-shot æ¨¡æ¿
few_shot_prompt = FewShotPromptTemplate(
    examples=examples,
    example_prompt=example_prompt,
    prefix="å°†æƒ…ç»ªè½¬æ¢ä¸º emojiï¼š",
    suffix="è¾“å…¥ï¼š{input}\nè¾“å‡ºï¼š",
    input_variables=["input"]
)

# ä½¿ç”¨
prompt = few_shot_prompt.format(input="æƒŠè®¶")
print(prompt)
```

---

### 4. Chainsï¼ˆé“¾å¼è°ƒç”¨ï¼‰

#### 4.1 LLMChain

```python
from langchain.chains import LLMChain

# åˆ›å»ºé“¾
prompt = PromptTemplate(
    input_variables=["topic"],
    template="å†™ä¸€é¦–å…³äº{topic}çš„è¯—ã€‚"
)

chain = LLMChain(llm=chat_model, prompt=prompt)

# æ‰§è¡Œ
result = chain.run("æ˜¥å¤©")
print(result)
```

---

#### 4.2 Sequential Chainï¼ˆé¡ºåºé“¾ï¼‰

```python
from langchain.chains import SimpleSequentialChain

# ç¬¬ä¸€ä¸ªé“¾ï¼šç”Ÿæˆå‰§æœ¬å¤§çº²
synopsis_chain = LLMChain(
    llm=chat_model,
    prompt=PromptTemplate(
        input_variables=["title"],
        template="ä¸ºç”µå½±ã€Š{title}ã€‹å†™ä¸€ä¸ªç®€çŸ­çš„å‰§æƒ…å¤§çº²ã€‚"
    )
)

# ç¬¬äºŒä¸ªé“¾ï¼šå†™è¯„è®º
review_chain = LLMChain(
    llm=chat_model,
    prompt=PromptTemplate(
        input_variables=["synopsis"],
        template="ä¸ºä»¥ä¸‹ç”µå½±å‰§æƒ…å†™ä¸€æ®µè¯„è®ºï¼š\n\n{synopsis}"
    )
)

# ç»„åˆé“¾
overall_chain = SimpleSequentialChain(
    chains=[synopsis_chain, review_chain],
    verbose=True  # æ˜¾ç¤ºä¸­é—´æ­¥éª¤
)

# æ‰§è¡Œ
result = overall_chain.run("æ—¶ç©ºæ—…è¡Œè€…")
print(result)
```

---

#### 4.3 LangChain Expression Language (LCEL)

```python
# ç°ä»£åŒ–çš„é“¾å¼è¯­æ³•
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# åˆ›å»ºé“¾ï¼ˆä½¿ç”¨ç®¡é“æ“ä½œç¬¦ |ï¼‰
chain = (
    {"topic": RunnablePassthrough()}
    | PromptTemplate.from_template("å†™ä¸€é¦–å…³äº{topic}çš„è¯—")
    | chat_model
    | StrOutputParser()
)

# æ‰§è¡Œ
result = chain.invoke("å¤å¤©")
print(result)
```

---

### 5. Memoryï¼ˆè®°å¿†ç®¡ç†ï¼‰

#### 5.1 ConversationBufferMemory

```python
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain

# åˆ›å»ºè®°å¿†
memory = ConversationBufferMemory()

# åˆ›å»ºå¯¹è¯é“¾
conversation = ConversationChain(
    llm=chat_model,
    memory=memory,
    verbose=True
)

# å¤šè½®å¯¹è¯
print(conversation.predict(input="æˆ‘å« Alice"))
print(conversation.predict(input="æˆ‘çš„åå­—æ˜¯ä»€ä¹ˆï¼Ÿ"))

# æŸ¥çœ‹è®°å¿†
print(memory.load_memory_variables({}))
```

---

#### 5.2 ConversationBufferWindowMemory

```python
from langchain.memory import ConversationBufferWindowMemory

# åªä¿ç•™æœ€è¿‘ k è½®å¯¹è¯
memory = ConversationBufferWindowMemory(k=2)

conversation = ConversationChain(
    llm=chat_model,
    memory=memory
)

# æµ‹è¯•
conversation.predict(input="ç¬¬ä¸€å¥è¯")
conversation.predict(input="ç¬¬äºŒå¥è¯")
conversation.predict(input="ç¬¬ä¸‰å¥è¯")
conversation.predict(input="ç¬¬å››å¥è¯")
conversation.predict(input="ä½ è¿˜è®°å¾—ç¬¬ä¸€å¥è¯å—ï¼Ÿ")  # åº”è¯¥ä¸è®°å¾—
```

---

#### 5.3 ConversationSummaryMemory

```python
from langchain.memory import ConversationSummaryMemory

# è‡ªåŠ¨æ€»ç»“å†å²å¯¹è¯
memory = ConversationSummaryMemory(llm=chat_model)

conversation = ConversationChain(
    llm=chat_model,
    memory=memory,
    verbose=True
)

# é•¿å¯¹è¯ä¼šè¢«è‡ªåŠ¨æ€»ç»“
conversation.predict(input="æˆ‘ä»Šå¹´ 25 å²ï¼Œæ˜¯ä¸€åå‰ç«¯å·¥ç¨‹å¸ˆã€‚")
conversation.predict(input="æˆ‘æ­£åœ¨å­¦ä¹  Python å’Œ AIã€‚")
conversation.predict(input="æˆ‘å…³äºæˆ‘çš„ä¿¡æ¯ä½ è®°å¾—å¤šå°‘ï¼Ÿ")

# æŸ¥çœ‹æ€»ç»“
print(memory.load_memory_variables({}))
```

---

### 6. Document Loadersï¼ˆæ–‡æ¡£åŠ è½½ï¼‰

#### 6.1 æ–‡æœ¬æ–‡ä»¶åŠ è½½

```python
from langchain.document_loaders import TextLoader

loader = TextLoader("data.txt", encoding="utf-8")
documents = loader.load()

print(f"åŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£")
print(documents[0].page_content[:200])  # å‰ 200 å­—ç¬¦
```

---

#### 6.2 PDF æ–‡æ¡£åŠ è½½

```python
from langchain.document_loaders import PyPDFLoader

loader = PyPDFLoader("document.pdf")
pages = loader.load_and_split()

print(f"PDF æœ‰ {len(pages)} é¡µ")
print(f"ç¬¬ä¸€é¡µå†…å®¹ï¼š{pages[0].page_content[:200]}")
```

---

#### 6.3 ç½‘é¡µåŠ è½½

```python
from langchain.document_loaders import WebBaseLoader

loader = WebBaseLoader("https://example.com")
documents = loader.load()

print(documents[0].page_content[:200])
```

---

### 7. Text Splittersï¼ˆæ–‡æœ¬åˆ†å—ï¼‰

#### 7.1 CharacterTextSplitter

```python
from langchain.text_splitter import CharacterTextSplitter

text_splitter = CharacterTextSplitter(
    chunk_size=1000,      # æ¯å—æœ€å¤§å­—ç¬¦æ•°
    chunk_overlap=200,    # é‡å å­—ç¬¦æ•°
    separator="\n\n"      # åˆ†éš”ç¬¦
)

# åˆ†å‰²æ–‡æœ¬
texts = text_splitter.split_text(long_text)
print(f"åˆ†å‰²æˆ {len(texts)} å—")
```

---

#### 7.2 RecursiveCharacterTextSplitter

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

# é€’å½’åˆ†å‰²ï¼ˆæ›´æ™ºèƒ½ï¼‰
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    separators=["\n\n", "\n", "ã€‚", " ", ""]  # ä¼˜å…ˆçº§ä»é«˜åˆ°ä½
)

# åˆ†å‰²æ–‡æ¡£
documents = text_splitter.split_documents(loaded_documents)
```

---

### 8. Vector Storesï¼ˆå‘é‡å­˜å‚¨ï¼‰

#### 8.1 Chromaï¼ˆæœ¬åœ°å‘é‡æ•°æ®åº“ï¼‰

```python
from langchain.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings

# åˆ›å»º embeddings
embeddings = OpenAIEmbeddings()

# åˆ›å»ºå‘é‡å­˜å‚¨
texts = [
    "LangChain æ˜¯ä¸€ä¸ª LLM åº”ç”¨æ¡†æ¶",
    "Python æ˜¯ä¸€é—¨ç¼–ç¨‹è¯­è¨€",
    "FastAPI æ˜¯ä¸€ä¸ª Web æ¡†æ¶"
]

vectorstore = Chroma.from_texts(
    texts=texts,
    embedding=embeddings,
    persist_directory="./chroma_db"  # æŒä¹…åŒ–å­˜å‚¨
)

# ç›¸ä¼¼åº¦æœç´¢
results = vectorstore.similarity_search("ä»€ä¹ˆæ˜¯ LangChainï¼Ÿ", k=2)
for doc in results:
    print(doc.page_content)
```

---

#### 8.2 FAISSï¼ˆé«˜æ€§èƒ½å‘é‡æ£€ç´¢ï¼‰

```python
from langchain.vectorstores import FAISS

# åˆ›å»º FAISS ç´¢å¼•
vectorstore = FAISS.from_texts(texts, embeddings)

# ä¿å­˜
vectorstore.save_local("faiss_index")

# åŠ è½½
loaded_vectorstore = FAISS.load_local(
    "faiss_index",
    embeddings,
    allow_dangerous_deserialization=True
)

# æ£€ç´¢
results = loaded_vectorstore.similarity_search("Python", k=1)
```

---

### 9. Retrieversï¼ˆæ£€ç´¢å™¨ï¼‰

```python
from langchain.chains import RetrievalQA

# åˆ›å»ºæ£€ç´¢å™¨
retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 3}  # è¿”å› 3 ä¸ªæœ€ç›¸ä¼¼çš„æ–‡æ¡£
)

# åˆ›å»ºé—®ç­”é“¾
qa_chain = RetrievalQA.from_chain_type(
    llm=chat_model,
    chain_type="stuff",  # æˆ– "map_reduce", "refine"
    retriever=retriever,
    return_source_documents=True  # è¿”å›æ¥æºæ–‡æ¡£
)

# æé—®
result = qa_chain({"query": "LangChain æ˜¯ä»€ä¹ˆï¼Ÿ"})
print(f"ç­”æ¡ˆï¼š{result['result']}")
print(f"æ¥æºï¼š{result['source_documents']}")
```

---

### 10. Output Parsersï¼ˆè¾“å‡ºè§£æï¼‰

#### 10.1 ç»“æ„åŒ–è¾“å‡º

```python
from langchain.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field

# å®šä¹‰è¾“å‡ºç»“æ„
class Person(BaseModel):
    name: str = Field(description="äººå")
    age: int = Field(description="å¹´é¾„")
    occupation: str = Field(description="èŒä¸š")

# åˆ›å»ºè§£æå™¨
parser = PydanticOutputParser(pydantic_object=Person)

# åˆ›å»ºæç¤ºè¯ï¼ˆåŒ…å«æ ¼å¼è¯´æ˜ï¼‰
prompt = PromptTemplate(
    template="æå–ä»¥ä¸‹æ–‡æœ¬ä¸­çš„äººç‰©ä¿¡æ¯ã€‚\n{format_instructions}\n\næ–‡æœ¬ï¼š{text}",
    input_variables=["text"],
    partial_variables={"format_instructions": parser.get_format_instructions()}
)

# åˆ›å»ºé“¾
chain = prompt | chat_model | parser

# æ‰§è¡Œ
result = chain.invoke({"text": "Alice ä»Šå¹´ 25 å²ï¼Œæ˜¯ä¸€åè½¯ä»¶å·¥ç¨‹å¸ˆã€‚"})
print(type(result))  # <class 'Person'>
print(result.name, result.age, result.occupation)
```

---

### 11. å®Œæ•´é¡¹ç›®ç¤ºä¾‹

#### é¡¹ç›®ï¼šæ™ºèƒ½æ–‡æ¡£é—®ç­”ç³»ç»Ÿ

```python
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.document_loaders import PyPDFLoader, TextLoader
import os

class DocumentQA:
    def __init__(self, model="gpt-3.5-turbo"):
        self.llm = ChatOpenAI(model=model, temperature=0)
        self.embeddings = OpenAIEmbeddings()
        self.vectorstore = None
        self.qa_chain = None
    
    def load_documents(self, file_paths):
        """åŠ è½½æ–‡æ¡£"""
        documents = []
        for file_path in file_paths:
            if file_path.endswith('.pdf'):
                loader = PyPDFLoader(file_path)
            elif file_path.endswith('.txt'):
                loader = TextLoader(file_path, encoding='utf-8')
            else:
                continue
            
            documents.extend(loader.load())
        
        # åˆ†å—
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        splits = text_splitter.split_documents(documents)
        
        # åˆ›å»ºå‘é‡å­˜å‚¨
        self.vectorstore = Chroma.from_documents(
            documents=splits,
            embedding=self.embeddings,
            persist_directory="./doc_qa_db"
        )
        
        # åˆ›å»ºé—®ç­”é“¾
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=self.vectorstore.as_retriever(search_kwargs={"k": 3}),
            return_source_documents=True
        )
        
        return f"æˆåŠŸåŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£ï¼Œåˆ†å‰²æˆ {len(splits)} ä¸ªå—"
    
    def ask(self, question):
        """æé—®"""
        if not self.qa_chain:
            return "è¯·å…ˆåŠ è½½æ–‡æ¡£"
        
        result = self.qa_chain({"query": question})
        
        answer = result['result']
        sources = [doc.metadata.get('source', 'Unknown') for doc in result['source_documents']]
        
        return {
            "answer": answer,
            "sources": list(set(sources))  # å»é‡
        }

# ä½¿ç”¨
doc_qa = DocumentQA()
print(doc_qa.load_documents(["document1.pdf", "document2.txt"]))

response = doc_qa.ask("è¿™ä»½æ–‡æ¡£çš„ä¸»è¦å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ")
print(f"ç­”æ¡ˆï¼š{response['answer']}")
print(f"æ¥æºï¼š{response['sources']}")
```

---

## ğŸ¯ å®æˆ˜ç»ƒä¹ 

### ç»ƒä¹  1ï¼šä¸ªäººçŸ¥è¯†åº“åŠ©æ‰‹
```python
# åŠ è½½ä¸ªäººç¬”è®°ã€æ–‡æ¡£
# å®ç°æ™ºèƒ½æœç´¢å’Œé—®ç­”
# æ”¯æŒå¯¹è¯å¼äº¤äº’
```

### ç»ƒä¹  2ï¼šä»£ç æ–‡æ¡£åŠ©æ‰‹
```python
# åŠ è½½ä»£ç åº“æ–‡æ¡£
# å›ç­”ä»£ç ç›¸å…³é—®é¢˜
# ç”Ÿæˆä»£ç ç¤ºä¾‹
```

### ç»ƒä¹  3ï¼šå¤šæ–‡æ¡£æ€»ç»“
```python
# åŠ è½½å¤šä¸ªæ–‡æ¡£
# ç”Ÿæˆç»¼åˆæ€»ç»“
# å¯¹æ¯”åˆ†æ
```

---

## ğŸ“– æ¨èèµ„æº

### å®˜æ–¹æ–‡æ¡£
- **LangChain æ–‡æ¡£**ï¼šhttps://python.langchain.com/
- **LangChain GitHub**ï¼šhttps://github.com/langchain-ai/langchain

### å­¦ä¹ èµ„æº
- **LangChain Cookbook**ï¼šå®˜æ–¹ç¤ºä¾‹é›†åˆ
- **LangChain Tutorialï¼ˆYouTubeï¼‰**ï¼šè§†é¢‘æ•™ç¨‹

### ç¤¾åŒº
- **LangChain Discord**ï¼šå®˜æ–¹ç¤¾åŒº
- **GitHub Discussions**ï¼šé—®é¢˜è®¨è®º

---

## âœ… å­¦ä¹ æ£€æŸ¥æ¸…å•

- [ ] ç†è§£ LangChain çš„æ ¸å¿ƒæ¦‚å¿µå’Œæ¶æ„
- [ ] æŒæ¡ Modelsã€Promptsã€Chains çš„ä½¿ç”¨
- [ ] èƒ½å¤Ÿå®ç°å¯¹è¯è®°å¿†ç®¡ç†
- [ ] æŒæ¡æ–‡æ¡£åŠ è½½å’Œæ–‡æœ¬åˆ†å—
- [ ] èƒ½å¤Ÿä½¿ç”¨å‘é‡æ•°æ®åº“è¿›è¡Œæ£€ç´¢
- [ ] ç†è§£å¹¶ä½¿ç”¨ Output Parsers
- [ ] å®Œæˆè‡³å°‘ 1 ä¸ªå®Œæ•´çš„æ–‡æ¡£é—®ç­”é¡¹ç›®

---

**ä¸‹ä¸€æ­¥**ï¼šå­¦ä¹  [05-RAGç³»ç»Ÿå¼€å‘](./05-RAGç³»ç»Ÿå¼€å‘.md)
