# 05 - RAG ç³»ç»Ÿå¼€å‘

> **å‰ç½®æ¡ä»¶**ï¼šå®Œæˆ LangChain æ¡†æ¶å­¦ä¹   
> **å­¦ä¹ æ—¶é•¿**ï¼š2-3 å‘¨  
> **å­¦ä¹ ç›®æ ‡**ï¼šæ·±å…¥ç†è§£ RAG åŸç†ï¼Œèƒ½å¤Ÿæ„å»ºç”Ÿäº§çº§ RAG ç³»ç»Ÿ

---

## ğŸ¯ ä»€ä¹ˆæ˜¯ RAGï¼Ÿ

### RAGï¼ˆRetrieval-Augmented Generationï¼‰æ£€ç´¢å¢å¼ºç”Ÿæˆ

**æ ¸å¿ƒæ€æƒ³**ï¼š
```
ç”¨æˆ·é—®é¢˜ â†’ æ£€ç´¢ç›¸å…³æ–‡æ¡£ â†’ å°†æ–‡æ¡£ä½œä¸ºä¸Šä¸‹æ–‡ â†’ LLM ç”Ÿæˆå›ç­”
```

**ä¸ºä»€ä¹ˆéœ€è¦ RAGï¼Ÿ**
- âœ… **è§£å†³ LLM çŸ¥è¯†è¿‡æ—¶é—®é¢˜**ï¼šå®æ—¶è·å–æœ€æ–°ä¿¡æ¯
- âœ… **å‡å°‘å¹»è§‰**ï¼šåŸºäºçœŸå®æ–‡æ¡£ç”Ÿæˆç­”æ¡ˆ
- âœ… **æ”¯æŒç§æœ‰æ•°æ®**ï¼šä¼ä¸šå†…éƒ¨æ–‡æ¡£ã€ä¸ªäººç¬”è®°
- âœ… **é™ä½æˆæœ¬**ï¼šæ— éœ€é‡æ–°è®­ç»ƒæ¨¡å‹

---

## ğŸ“š RAG ç³»ç»Ÿæ¶æ„

### åŸºç¡€æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  æ–‡æ¡£åŠ è½½    â”‚ â†’ PDFã€Wordã€ç½‘é¡µã€æ•°æ®åº“
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  æ–‡æœ¬åˆ†å—    â”‚ â†’ æ™ºèƒ½åˆ†å‰²ï¼Œä¿æŒè¯­ä¹‰å®Œæ•´
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  å‘é‡åŒ–      â”‚ â†’ Embeddings æ¨¡å‹
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  å‘é‡å­˜å‚¨    â”‚ â†’ Chromaã€Pineconeã€Weaviate
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ç”¨æˆ·é—®é¢˜    â”‚ â”€â”€â†’ â”‚  å‘é‡æ£€ç´¢    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                           â†“
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  ç”Ÿæˆå›ç­”    â”‚ â†’ GPT-4
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“š å­¦ä¹ å†…å®¹

### 1. æ–‡æ¡£åˆ†å—ç­–ç•¥

#### 1.1 åŸºç¡€åˆ†å—

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,        # å—å¤§å°
    chunk_overlap=200,      # é‡å éƒ¨åˆ†
    length_function=len,    # é•¿åº¦è®¡ç®—å‡½æ•°
    separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", " ", ""]
)

chunks = text_splitter.split_text(long_text)
```

---

#### 1.2 æ™ºèƒ½åˆ†å—ï¼ˆæ ¹æ®å†…å®¹ç»“æ„ï¼‰

```python
from langchain.text_splitter import MarkdownHeaderTextSplitter

# Markdown ç»“æ„åŒ–åˆ†å—
markdown_splitter = MarkdownHeaderTextSplitter(
    headers_to_split_on=[
        ("#", "Header 1"),
        ("##", "Header 2"),
        ("###", "Header 3"),
    ]
)

# ä¿ç•™æ–‡æ¡£ç»“æ„
md_header_splits = markdown_splitter.split_text(markdown_text)
```

---

#### 1.3 è¯­ä¹‰åˆ†å—

```python
from langchain.text_splitter import SemanticChunker
from langchain_openai import OpenAIEmbeddings

# åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦åˆ†å—
semantic_chunker = SemanticChunker(
    OpenAIEmbeddings(),
    breakpoint_threshold_type="percentile",  # æˆ– "standard_deviation"
    breakpoint_threshold_amount=0.95
)

semantic_chunks = semantic_chunker.split_text(text)
```

---

### 2. å‘é‡æ•°æ®åº“å¯¹æ¯”

| æ•°æ®åº“ | ç±»å‹ | ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |
|--------|------|------|---------|
| **Chroma** | æœ¬åœ°/äº‘ | è½»é‡çº§ã€æ˜“ç”¨ | å¼€å‘æµ‹è¯• |
| **FAISS** | æœ¬åœ° | é«˜æ€§èƒ½ã€å†…å­˜ | é«˜æ€§èƒ½æ£€ç´¢ |
| **Pinecone** | äº‘æœåŠ¡ | æ‰˜ç®¡ã€å¯æ‰©å±• | ç”Ÿäº§ç¯å¢ƒ |
| **Weaviate** | æœ¬åœ°/äº‘ | GraphQLã€æ··åˆæœç´¢ | å¤æ‚æŸ¥è¯¢ |
| **Milvus** | æœ¬åœ°/äº‘ | é«˜æ€§èƒ½ã€åˆ†å¸ƒå¼ | å¤§è§„æ¨¡åº”ç”¨ |

---

### 3. å‘é‡æ£€ç´¢ä¼˜åŒ–

#### 3.1 æ··åˆæ£€ç´¢ï¼ˆHybrid Searchï¼‰

```python
from langchain.retrievers import EnsembleRetriever
from langchain.retrievers import BM25Retriever
from langchain.vectorstores import Chroma

# å‘é‡æ£€ç´¢å™¨
vectorstore = Chroma.from_documents(documents, embeddings)
vector_retriever = vectorstore.as_retriever(search_kwargs={"k": 5})

# å…³é”®è¯æ£€ç´¢å™¨ï¼ˆBM25ï¼‰
bm25_retriever = BM25Retriever.from_documents(documents)
bm25_retriever.k = 5

# é›†æˆæ£€ç´¢å™¨ï¼ˆæ··åˆï¼‰
ensemble_retriever = EnsembleRetriever(
    retrievers=[vector_retriever, bm25_retriever],
    weights=[0.5, 0.5]  # æƒé‡åˆ†é…
)

# æ£€ç´¢
results = ensemble_retriever.get_relevant_documents("æŸ¥è¯¢å†…å®¹")
```

---

#### 3.2 é‡æ’åºï¼ˆRe-rankingï¼‰

```python
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor

# åŸºç¡€æ£€ç´¢å™¨
base_retriever = vectorstore.as_retriever(search_kwargs={"k": 10})

# LLM å‹ç¼©å™¨ï¼ˆé‡æ’åºï¼‰
compressor = LLMChainExtractor.from_llm(llm)

# å‹ç¼©æ£€ç´¢å™¨
compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=base_retriever
)

# åªè¿”å›æœ€ç›¸å…³çš„å†…å®¹
compressed_docs = compression_retriever.get_relevant_documents("æŸ¥è¯¢å†…å®¹")
```

---

#### 3.3 å¤šæŸ¥è¯¢æ£€ç´¢

```python
from langchain.retrievers.multi_query import MultiQueryRetriever

# è‡ªåŠ¨ç”Ÿæˆå¤šä¸ªç›¸å…³æŸ¥è¯¢
multi_query_retriever = MultiQueryRetriever.from_llm(
    retriever=vectorstore.as_retriever(),
    llm=llm
)

# ä¸€ä¸ªé—®é¢˜ï¼Œå¤šä¸ªè§’åº¦æ£€ç´¢
results = multi_query_retriever.get_relevant_documents(
    "Python çš„ä¼˜åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ"
)
# LLM ä¼šè‡ªåŠ¨ç”Ÿæˆï¼š
# - "Python çš„å¥½å¤„æœ‰å“ªäº›ï¼Ÿ"
# - "ä¸ºä»€ä¹ˆé€‰æ‹© Pythonï¼Ÿ"
# - "Python ç›¸æ¯”å…¶ä»–è¯­è¨€çš„ä¼˜ç‚¹"
```

---

### 4. RAG é“¾ç±»å‹

#### 4.1 Stuff Chainï¼ˆå¡«å……é“¾ï¼‰

```python
from langchain.chains import RetrievalQA

# æœ€ç®€å•ï¼šå°†æ‰€æœ‰æ–‡æ¡£å¡«å……åˆ°æç¤ºè¯ä¸­
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",  # é€‚åˆæ–‡æ¡£æ•°é‡å°‘
    retriever=retriever
)

result = qa_chain.run("é—®é¢˜")
```

**ä¼˜ç‚¹**ï¼šç®€å•ã€å¿«é€Ÿ  
**ç¼ºç‚¹**ï¼šæ–‡æ¡£å¤šæ—¶è¶…å‡ºä¸Šä¸‹æ–‡çª—å£

---

#### 4.2 Map-Reduce Chain

```python
# åˆ†åˆ«å¤„ç†æ¯ä¸ªæ–‡æ¡£ï¼Œç„¶ååˆå¹¶ç»“æœ
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="map_reduce",  # é€‚åˆå¤§é‡æ–‡æ¡£
    retriever=retriever
)
```

**å·¥ä½œæµç¨‹**ï¼š
```
æ–‡æ¡£1 â†’ LLM â†’ ç­”æ¡ˆ1 â”
æ–‡æ¡£2 â†’ LLM â†’ ç­”æ¡ˆ2 â”œâ†’ LLM â†’ æœ€ç»ˆç­”æ¡ˆ
æ–‡æ¡£3 â†’ LLM â†’ ç­”æ¡ˆ3 â”˜
```

**ä¼˜ç‚¹**ï¼šå¯å¤„ç†å¤§é‡æ–‡æ¡£  
**ç¼ºç‚¹**ï¼šå¤šæ¬¡ LLM è°ƒç”¨ï¼Œæˆæœ¬é«˜

---

#### 4.3 Refine Chain

```python
# è¿­ä»£ä¼˜åŒ–ç­”æ¡ˆ
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="refine",  # é€æ­¥ä¼˜åŒ–ç­”æ¡ˆ
    retriever=retriever
)
```

**å·¥ä½œæµç¨‹**ï¼š
```
åˆå§‹ç­”æ¡ˆ + æ–‡æ¡£1 â†’ æ”¹è¿›ç­”æ¡ˆ1
æ”¹è¿›ç­”æ¡ˆ1 + æ–‡æ¡£2 â†’ æ”¹è¿›ç­”æ¡ˆ2
æ”¹è¿›ç­”æ¡ˆ2 + æ–‡æ¡£3 â†’ æœ€ç»ˆç­”æ¡ˆ
```

---

### 5. é«˜çº§ RAG æŠ€æœ¯

#### 5.1 Parent Document Retriever

```python
from langchain.retrievers import ParentDocumentRetriever
from langchain.storage import InMemoryStore

# å°å—æ£€ç´¢ï¼Œå¤§å—è¿”å›
parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)
child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)

vectorstore = Chroma(embedding_function=embeddings)
store = InMemoryStore()

retriever = ParentDocumentRetriever(
    vectorstore=vectorstore,
    docstore=store,
    child_splitter=child_splitter,
    parent_splitter=parent_splitter,
)

retriever.add_documents(documents)
```

**ä¼˜ç‚¹**ï¼šæ£€ç´¢ç²¾ç¡®ï¼ˆå°å—ï¼‰ï¼Œä¸Šä¸‹æ–‡å®Œæ•´ï¼ˆå¤§å—ï¼‰

---

#### 5.2 Self-Query Retriever

```python
from langchain.retrievers.self_query.base import SelfQueryRetriever
from langchain.chains.query_constructor.base import AttributeInfo

# å…ƒæ•°æ®å®šä¹‰
metadata_field_info = [
    AttributeInfo(
        name="author",
        description="æ–‡æ¡£ä½œè€…",
        type="string"
    ),
    AttributeInfo(
        name="year",
        description="å‘å¸ƒå¹´ä»½",
        type="integer"
    ),
]

document_content_description = "æŠ€æœ¯æ–‡æ¡£é›†åˆ"

# è‡ªæŸ¥è¯¢æ£€ç´¢å™¨
retriever = SelfQueryRetriever.from_llm(
    llm=llm,
    vectorstore=vectorstore,
    document_contents=document_content_description,
    metadata_field_info=metadata_field_info,
    verbose=True
)

# è‡ªåŠ¨ä»é—®é¢˜ä¸­æå–è¿‡æ»¤æ¡ä»¶
results = retriever.get_relevant_documents(
    "2023 å¹´å…³äº Python çš„æ–‡æ¡£"
)
```

---

### 6. RAG è¯„ä¼°

#### 6.1 è¯„ä¼°æŒ‡æ ‡

```python
from ragas import evaluate
from ragas.metrics import (
    faithfulness,          # å¿ å®åº¦ï¼ˆç­”æ¡ˆæ˜¯å¦åŸºäºæ–‡æ¡£ï¼‰
    answer_relevancy,      # ç­”æ¡ˆç›¸å…³æ€§
    context_precision,     # ä¸Šä¸‹æ–‡ç²¾ç¡®åº¦
    context_recall,        # ä¸Šä¸‹æ–‡å¬å›ç‡
)

# è¯„ä¼°æ•°æ®é›†
eval_dataset = {
    "question": ["é—®é¢˜1", "é—®é¢˜2"],
    "answer": ["ç­”æ¡ˆ1", "ç­”æ¡ˆ2"],
    "contexts": [["æ–‡æ¡£1", "æ–‡æ¡£2"], ["æ–‡æ¡£3"]],
    "ground_truths": [["æ ‡å‡†ç­”æ¡ˆ1"], ["æ ‡å‡†ç­”æ¡ˆ2"]]
}

# è¿è¡Œè¯„ä¼°
result = evaluate(
    dataset=eval_dataset,
    metrics=[
        faithfulness,
        answer_relevancy,
        context_precision,
        context_recall,
    ],
)

print(result)
```

---

#### 6.2 è‡ªå®šä¹‰è¯„ä¼°

```python
class RAGEvaluator:
    def __init__(self, qa_chain, llm):
        self.qa_chain = qa_chain
        self.llm = llm
    
    def evaluate_answer_quality(self, question, answer, ground_truth):
        """è¯„ä¼°ç­”æ¡ˆè´¨é‡"""
        prompt = f"""
        é—®é¢˜ï¼š{question}
        ç”Ÿæˆç­”æ¡ˆï¼š{answer}
        æ ‡å‡†ç­”æ¡ˆï¼š{ground_truth}
        
        è¯·è¯„ä¼°ç”Ÿæˆç­”æ¡ˆçš„è´¨é‡ï¼ˆ0-10åˆ†ï¼‰ï¼Œå¹¶è¯´æ˜ç†ç”±ã€‚
        """
        
        response = self.llm.invoke(prompt)
        return response.content
    
    def evaluate_retrieval(self, question, retrieved_docs, relevant_docs):
        """è¯„ä¼°æ£€ç´¢è´¨é‡"""
        retrieved_ids = set([doc.metadata.get('id') for doc in retrieved_docs])
        relevant_ids = set(relevant_docs)
        
        precision = len(retrieved_ids & relevant_ids) / len(retrieved_ids)
        recall = len(retrieved_ids & relevant_ids) / len(relevant_ids)
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        
        return {
            "precision": precision,
            "recall": recall,
            "f1_score": f1
        }
```

---

### 7. ç”Ÿäº§çº§ RAG ç³»ç»Ÿ

#### 7.1 å®Œæ•´å®ç°

```python
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.document_loaders import DirectoryLoader, PyPDFLoader
import os

class ProductionRAG:
    def __init__(
        self,
        model="gpt-4",
        chunk_size=1000,
        chunk_overlap=200,
        k_documents=5
    ):
        self.llm = ChatOpenAI(model=model, temperature=0)
        self.embeddings = OpenAIEmbeddings()
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.k_documents = k_documents
        
        self.vectorstore = None
        self.qa_chain = None
        self.memory = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True,
            output_key="answer"
        )
    
    def load_directory(self, directory_path):
        """åŠ è½½ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡æ¡£"""
        # PDF æ–‡æ¡£
        pdf_loader = DirectoryLoader(
            directory_path,
            glob="**/*.pdf",
            loader_cls=PyPDFLoader
        )
        
        # æ–‡æœ¬æ–‡æ¡£
        txt_loader = DirectoryLoader(
            directory_path,
            glob="**/*.txt",
            loader_cls=lambda path: TextLoader(path, encoding='utf-8')
        )
        
        documents = []
        documents.extend(pdf_loader.load())
        documents.extend(txt_loader.load())
        
        # åˆ†å—
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", " ", ""]
        )
        
        splits = text_splitter.split_documents(documents)
        
        # åˆ›å»ºå‘é‡å­˜å‚¨
        self.vectorstore = Chroma.from_documents(
            documents=splits,
            embedding=self.embeddings,
            persist_directory="./production_rag_db"
        )
        
        # åˆ›å»ºé—®ç­”é“¾
        self.qa_chain = ConversationalRetrievalChain.from_llm(
            llm=self.llm,
            retriever=self.vectorstore.as_retriever(
                search_type="mmr",  # Maximum Marginal Relevance
                search_kwargs={"k": self.k_documents, "fetch_k": 20}
            ),
            memory=self.memory,
            return_source_documents=True,
            verbose=True
        )
        
        return f"æˆåŠŸåŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£ï¼Œåˆ†å‰²æˆ {len(splits)} ä¸ªå—"
    
    def ask(self, question):
        """æé—®"""
        if not self.qa_chain:
            return {"error": "è¯·å…ˆåŠ è½½æ–‡æ¡£"}
        
        result = self.qa_chain({"question": question})
        
        return {
            "answer": result['answer'],
            "sources": [
                {
                    "content": doc.page_content[:200],
                    "metadata": doc.metadata
                }
                for doc in result['source_documents']
            ]
        }
    
    def clear_memory(self):
        """æ¸…ç©ºå¯¹è¯å†å²"""
        self.memory.clear()
    
    def add_documents(self, new_documents):
        """åŠ¨æ€æ·»åŠ æ–‡æ¡£"""
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap
        )
        
        splits = text_splitter.split_documents(new_documents)
        self.vectorstore.add_documents(splits)
        
        return f"æ·»åŠ äº† {len(splits)} ä¸ªæ–°æ–‡æ¡£å—"

# ä½¿ç”¨ç¤ºä¾‹
rag_system = ProductionRAG(model="gpt-4", k_documents=5)

# åŠ è½½æ–‡æ¡£
print(rag_system.load_directory("./documents"))

# æé—®
response = rag_system.ask("è¿™äº›æ–‡æ¡£çš„ä¸»è¦å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ")
print(f"ç­”æ¡ˆï¼š{response['answer']}")
print(f"æ¥æºï¼š{response['sources']}")

# ç»§ç»­å¯¹è¯
response = rag_system.ask("èƒ½è¯¦ç»†è§£é‡Šä¸€ä¸‹å—ï¼Ÿ")
print(f"ç­”æ¡ˆï¼š{response['answer']}")
```

---

#### 7.2 FastAPI é›†æˆ

```python
from fastapi import FastAPI, HTTPException, UploadFile, File
from pydantic import BaseModel
import shutil

app = FastAPI()
rag_system = ProductionRAG()

class Question(BaseModel):
    question: str

@app.post("/upload")
async def upload_document(file: UploadFile = File(...)):
    """ä¸Šä¼ æ–‡æ¡£"""
    try:
        # ä¿å­˜æ–‡ä»¶
        file_path = f"./uploads/{file.filename}"
        with open(file_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        
        # åŠ è½½æ–‡æ¡£
        loader = PyPDFLoader(file_path) if file.filename.endswith('.pdf') else TextLoader(file_path)
        documents = loader.load()
        result = rag_system.add_documents(documents)
        
        return {"message": result}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/ask")
async def ask_question(question: Question):
    """æé—®"""
    try:
        response = rag_system.ask(question.question)
        return response
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/memory")
async def clear_memory():
    """æ¸…ç©ºå¯¹è¯å†å²"""
    rag_system.clear_memory()
    return {"message": "å¯¹è¯å†å²å·²æ¸…ç©º"}
```

---

### 8. RAG ä¼˜åŒ–æŠ€å·§

#### 8.1 æç¤ºè¯ä¼˜åŒ–

```python
# è‡ªå®šä¹‰ RAG æç¤ºè¯
from langchain.prompts import PromptTemplate

template = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£åˆ†æåŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ã€‚

è§„åˆ™ï¼š
1. åªä½¿ç”¨æä¾›çš„æ–‡æ¡£å†…å®¹å›ç­”
2. å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œæ˜ç¡®è¯´æ˜
3. å¼•ç”¨å…·ä½“çš„æ–‡æ¡£å†…å®¹
4. ä¿æŒå›ç­”ç®€æ´å‡†ç¡®

æ–‡æ¡£å†…å®¹ï¼š
{context}

é—®é¢˜ï¼š{question}

å›ç­”ï¼š
"""

PROMPT = PromptTemplate(
    template=template,
    input_variables=["context", "question"]
)

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    chain_type_kwargs={"prompt": PROMPT}
)
```

---

#### 8.2 å…ƒæ•°æ®è¿‡æ»¤

```python
# æ·»åŠ å…ƒæ•°æ®
documents = [
    Document(
        page_content="å†…å®¹",
        metadata={
            "source": "document.pdf",
            "page": 1,
            "category": "æŠ€æœ¯",
            "date": "2024-01-01"
        }
    )
]

# æ ¹æ®å…ƒæ•°æ®è¿‡æ»¤
retriever = vectorstore.as_retriever(
    search_kwargs={
        "k": 5,
        "filter": {"category": "æŠ€æœ¯"}
    }
)
```

---

#### 8.3 ç¼“å­˜ä¼˜åŒ–

```python
from langchain.cache import InMemoryCache
from langchain.globals import set_llm_cache

# å¯ç”¨ç¼“å­˜
set_llm_cache(InMemoryCache())

# ç›¸åŒé—®é¢˜ä¸ä¼šé‡å¤è°ƒç”¨ LLM
result1 = qa_chain.run("ä»€ä¹ˆæ˜¯ Pythonï¼Ÿ")
result2 = qa_chain.run("ä»€ä¹ˆæ˜¯ Pythonï¼Ÿ")  # ä½¿ç”¨ç¼“å­˜
```

---

## ğŸ¯ å®æˆ˜é¡¹ç›®

### é¡¹ç›®ï¼šä¼ä¸šçŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿ

**åŠŸèƒ½éœ€æ±‚**ï¼š
- æ”¯æŒå¤šç§æ–‡æ¡£æ ¼å¼ï¼ˆPDFã€Wordã€TXTã€Markdownï¼‰
- å®æ—¶æ–‡æ¡£ä¸Šä¼ å’Œç´¢å¼•
- å¤šè½®å¯¹è¯æ”¯æŒ
- æ¥æºè¿½æº¯
- æœç´¢å†å²è®°å½•
- æƒé™ç®¡ç†

**æŠ€æœ¯æ ˆ**ï¼š
- åç«¯ï¼šFastAPI + LangChain
- å‘é‡æ•°æ®åº“ï¼šChroma æˆ– Pinecone
- å‰ç«¯ï¼šReact + TypeScript
- éƒ¨ç½²ï¼šDocker + AWS/Azure

---

## ğŸ“– æ¨èèµ„æº

### å­¦ä¹ èµ„æº
- **LangChain RAG æ–‡æ¡£**ï¼šhttps://python.langchain.com/docs/use_cases/question_answering
- **RAG Survey Paper**ï¼šæœ€æ–° RAG ç ”ç©¶ç»¼è¿°
- **Building RAG from Scratch**ï¼šYouTube æ•™ç¨‹

### å·¥å…·ä¸åº“
- **LlamaIndex**ï¼šä¸“æ³¨äº RAG çš„æ¡†æ¶
- **Haystack**ï¼šå¦ä¸€ä¸ª RAG æ¡†æ¶
- **RAGAS**ï¼šRAG è¯„ä¼°å·¥å…·

---

## âœ… å­¦ä¹ æ£€æŸ¥æ¸…å•

- [ ] ç†è§£ RAG çš„åŸºæœ¬åŸç†å’Œæ¶æ„
- [ ] æŒæ¡å¤šç§æ–‡æ¡£åˆ†å—ç­–ç•¥
- [ ] èƒ½å¤Ÿé€‰æ‹©åˆé€‚çš„å‘é‡æ•°æ®åº“
- [ ] å®ç°æ··åˆæ£€ç´¢å’Œé‡æ’åº
- [ ] ç†è§£ä¸åŒ RAG é“¾ç±»å‹çš„é€‚ç”¨åœºæ™¯
- [ ] æŒæ¡ RAG è¯„ä¼°æ–¹æ³•
- [ ] å®Œæˆä¸€ä¸ªç”Ÿäº§çº§ RAG ç³»ç»Ÿ

---

**ä¸‹ä¸€æ­¥**ï¼šå­¦ä¹  [06-Agentå¼€å‘å®æˆ˜](./06-Agentå¼€å‘å®æˆ˜.md)
