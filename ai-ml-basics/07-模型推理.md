# æ¨¡å‹æ¨ç†åŸºç¡€

## ğŸ“‹ ä»€ä¹ˆæ˜¯æ¨¡å‹æ¨ç†ï¼Ÿ

**æ¨¡å‹æ¨ç†ï¼ˆModel Inferenceï¼‰**æ˜¯æŒ‡ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹å¯¹æ–°æ•°æ®è¿›è¡Œé¢„æµ‹çš„è¿‡ç¨‹ã€‚

## ğŸ¯ æ¨ç† vs è®­ç»ƒ

| ç‰¹æ€§ | è®­ç»ƒï¼ˆTrainingï¼‰ | æ¨ç†ï¼ˆInferenceï¼‰ |
|------|-----------------|------------------|
| **ç›®çš„** | å­¦ä¹ æ¨¡å‹å‚æ•° | ä½¿ç”¨æ¨¡å‹é¢„æµ‹ |
| **æ•°æ®** | è®­ç»ƒæ•°æ®é›† | æ–°æ•°æ® |
| **è®¡ç®—** | å‰å‘ + åå‘ä¼ æ’­ | ä»…å‰å‘ä¼ æ’­ |
| **èµ„æº** | éœ€è¦å¤§é‡èµ„æº | ç›¸å¯¹è¾ƒå°‘ |
| **æ¨¡å¼** | æ‰¹é‡å¤„ç† | å®æ—¶/æ‰¹é‡ |

## ğŸ—ï¸ æ¨ç†æµç¨‹

### åŸºæœ¬æµç¨‹

```
è¾“å…¥æ•°æ®
    â†“
æ•°æ®é¢„å¤„ç†
    â†“
æ¨¡å‹å‰å‘ä¼ æ’­
    â†“
åå¤„ç†
    â†“
è¾“å‡ºç»“æœ
```

### è¯¦ç»†æ­¥éª¤

#### 1. æ•°æ®é¢„å¤„ç†

```python
# å›¾åƒé¢„å¤„ç†
image = load_image("test.jpg")
image = resize(image, (224, 224))
image = normalize(image)
image = to_tensor(image)

# æ–‡æœ¬é¢„å¤„ç†
text = "Hello, world!"
tokens = tokenize(text)
input_ids = encode(tokens)
```

#### 2. æ¨¡å‹æ¨ç†

```python
# è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
model.eval()

# ç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼ˆèŠ‚çœå†…å­˜ï¼‰
with torch.no_grad():
    output = model(input_data)
```

#### 3. åå¤„ç†

```python
# åˆ†ç±»ä»»åŠ¡
probabilities = softmax(output)
predicted_class = argmax(probabilities)

# å›å½’ä»»åŠ¡
predicted_value = output.item()
```

## ğŸ”§ æ¨ç†æ¨¡å¼

### 1. æ‰¹é‡æ¨ç†ï¼ˆBatch Inferenceï¼‰

```python
# ä¸€æ¬¡å¤„ç†å¤šä¸ªæ ·æœ¬
batch = [sample1, sample2, sample3, ...]
predictions = model(batch)
```

**ä¼˜åŠ¿**ï¼š
- æé«˜ GPU åˆ©ç”¨ç‡
- æé«˜ååé‡

### 2. å®æ—¶æ¨ç†ï¼ˆReal-time Inferenceï¼‰

```python
# é€ä¸ªå¤„ç†æ ·æœ¬
for sample in stream:
    prediction = model(sample)
    process(prediction)
```

**ä¼˜åŠ¿**ï¼š
- ä½å»¶è¿Ÿ
- é€‚åˆåœ¨çº¿æœåŠ¡

### 3. æµå¼æ¨ç†ï¼ˆStreaming Inferenceï¼‰

```python
# å¤„ç†æµå¼æ•°æ®
for chunk in data_stream:
    output = model.process_chunk(chunk)
    yield output
```

## ğŸ’¡ ä¼˜åŒ–æŠ€æœ¯

### 1. æ¨¡å‹é‡åŒ–ï¼ˆQuantizationï¼‰

```python
# å°† FP32 è½¬æ¢ä¸º INT8
quantized_model = quantize(model, dtype=torch.int8)
```

**ä¼˜åŠ¿**ï¼š
- å‡å°‘æ¨¡å‹å¤§å°
- åŠ é€Ÿæ¨ç†
- é™ä½å†…å­˜ä½¿ç”¨

### 2. æ¨¡å‹å‰ªæï¼ˆPruningï¼‰

```python
# ç§»é™¤ä¸é‡è¦çš„æƒé‡
pruned_model = prune(model, sparsity=0.5)
```

**ä¼˜åŠ¿**ï¼š
- å‡å°‘æ¨¡å‹å¤§å°
- åŠ é€Ÿæ¨ç†

### 3. æ¨¡å‹è’¸é¦ï¼ˆDistillationï¼‰

```python
# ä½¿ç”¨å¤§æ¨¡å‹è®­ç»ƒå°æ¨¡å‹
student_model = distill(teacher_model, student_model)
```

**ä¼˜åŠ¿**ï¼š
- ä¿æŒæ€§èƒ½
- å‡å°‘æ¨¡å‹å¤§å°

### 4. å›¾ä¼˜åŒ–ï¼ˆGraph Optimizationï¼‰

```python
# TensorRT ä¼˜åŒ–
optimized_model = tensorrt.optimize(model)
```

**ä¼˜åŠ¿**ï¼š
- èåˆæ“ä½œ
- å‡å°‘å†…å­˜è®¿é—®
- æé«˜é€Ÿåº¦

## ğŸ“ ä»£ç ç¤ºä¾‹

### PyTorch æ¨ç†

```python
import torch
import torch.nn as nn

# åŠ è½½æ¨¡å‹
model = load_model("model.pth")
model.eval()

# æ¨ç†
def inference(input_data):
    with torch.no_grad():
        output = model(input_data)
        return output

# ä½¿ç”¨
result = inference(test_data)
```

### ONNX æ¨ç†

```python
import onnxruntime as ort

# åŠ è½½ ONNX æ¨¡å‹
session = ort.InferenceSession("model.onnx")

# æ¨ç†
def inference(input_data):
    output = session.run(None, {"input": input_data})
    return output
```

### TensorFlow æ¨ç†

```python
import tensorflow as tf

# åŠ è½½æ¨¡å‹
model = tf.keras.models.load_model("model.h5")

# æ¨ç†
def inference(input_data):
    output = model.predict(input_data)
    return output
```

## ğŸ¯ éƒ¨ç½²æ–¹å¼

### 1. æœ¬åœ°éƒ¨ç½²

```python
# åœ¨æœ¬åœ°æœåŠ¡å™¨ä¸Šè¿è¡Œ
model = load_model()
server = create_server(model)
server.run(port=8080)
```

### 2. äº‘ç«¯éƒ¨ç½²

- **AWS SageMaker**
- **Google Cloud AI Platform**
- **Azure ML**

### 3. è¾¹ç¼˜éƒ¨ç½²

- **ç§»åŠ¨è®¾å¤‡**
- **IoT è®¾å¤‡**
- **åµŒå…¥å¼ç³»ç»Ÿ**

## âš¡ æ€§èƒ½ä¼˜åŒ–

### 1. æ‰¹å¤„ç†

```python
# æ‰¹é‡å¤„ç†æé«˜ååé‡
batch_size = 32
for i in range(0, len(data), batch_size):
    batch = data[i:i+batch_size]
    predictions = model(batch)
```

### 2. å¼‚æ­¥æ¨ç†

```python
import asyncio

async def async_inference(data):
    result = await model.predict_async(data)
    return result
```

### 3. ç¼“å­˜

```python
from functools import lru_cache

@lru_cache(maxsize=1000)
def cached_inference(input_hash):
    return model.predict(input_data)
```

## ğŸ”— ç›¸å…³æ¦‚å¿µ

- **Inference Engine**ï¼šæ¨ç†å¼•æ“
- **Model Serving**ï¼šæ¨¡å‹æœåŠ¡
- **Latency**ï¼šå»¶è¿Ÿ
- **Throughput**ï¼šååé‡
- **Model Optimization**ï¼šæ¨¡å‹ä¼˜åŒ–

---

*æœ€åæ›´æ–°ï¼š2024å¹´*
