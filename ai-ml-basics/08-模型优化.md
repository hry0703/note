# æ¨¡å‹ä¼˜åŒ–åŸºç¡€

## ğŸ“‹ ä»€ä¹ˆæ˜¯æ¨¡å‹ä¼˜åŒ–ï¼Ÿ

**æ¨¡å‹ä¼˜åŒ–**æ˜¯æŒ‡é€šè¿‡å„ç§æŠ€æœ¯æé«˜æ¨¡å‹çš„æ€§èƒ½ã€æ•ˆç‡å’Œå¯éƒ¨ç½²æ€§ã€‚

## ğŸ¯ ä¼˜åŒ–ç›®æ ‡

### 1. æ€§èƒ½ä¼˜åŒ–
- æé«˜å‡†ç¡®ç‡
- é™ä½æŸå¤±
- æ”¹å–„æ³›åŒ–èƒ½åŠ›

### 2. æ•ˆç‡ä¼˜åŒ–
- å‡å°‘æ¨¡å‹å¤§å°
- åŠ é€Ÿæ¨ç†
- é™ä½å†…å­˜ä½¿ç”¨

### 3. éƒ¨ç½²ä¼˜åŒ–
- é€‚é…ä¸åŒç¡¬ä»¶
- æ”¯æŒè¾¹ç¼˜è®¾å¤‡
- é™ä½å»¶è¿Ÿ

## ğŸ”§ ä¼˜åŒ–æŠ€æœ¯

### 1. æ¨¡å‹é‡åŒ–ï¼ˆQuantizationï¼‰

**å®šä¹‰**ï¼šå°†æ¨¡å‹å‚æ•°ä»é«˜ç²¾åº¦ï¼ˆFP32ï¼‰è½¬æ¢ä¸ºä½ç²¾åº¦ï¼ˆINT8ï¼‰

**ç±»å‹**ï¼š
- **è®­ç»ƒåé‡åŒ–**ï¼šè®­ç»ƒå®Œæˆåé‡åŒ–
- **é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ**ï¼šè®­ç»ƒæ—¶è€ƒè™‘é‡åŒ–

**ä¼˜åŠ¿**ï¼š
- æ¨¡å‹å¤§å°å‡å°‘ 4 å€
- æ¨ç†é€Ÿåº¦æå‡ 2-4 å€
- å†…å­˜ä½¿ç”¨å‡å°‘

**ç¤ºä¾‹**ï¼š
```python
import torch
import torch.quantization as quantization

# é‡åŒ–æ¨¡å‹
model_fp32 = load_model()
model_int8 = quantization.quantize_dynamic(
    model_fp32, 
    {torch.nn.Linear}, 
    dtype=torch.qint8
)
```

### 2. æ¨¡å‹å‰ªæï¼ˆPruningï¼‰

**å®šä¹‰**ï¼šç§»é™¤ä¸é‡è¦çš„æƒé‡æˆ–ç¥ç»å…ƒ

**ç±»å‹**ï¼š
- **ç»“æ„åŒ–å‰ªæ**ï¼šç§»é™¤æ•´ä¸ªé€šé“æˆ–å±‚
- **éç»“æ„åŒ–å‰ªæ**ï¼šç§»é™¤å•ä¸ªæƒé‡

**ä¼˜åŠ¿**ï¼š
- å‡å°‘æ¨¡å‹å¤§å°
- åŠ é€Ÿæ¨ç†
- é™ä½è®¡ç®—é‡

**ç¤ºä¾‹**ï¼š
```python
import torch.nn.utils.prune as prune

# å‰ªæ 50% çš„æƒé‡
prune.l1_unstructured(
    module, 
    name='weight', 
    amount=0.5
)
```

### 3. çŸ¥è¯†è’¸é¦ï¼ˆKnowledge Distillationï¼‰

**å®šä¹‰**ï¼šä½¿ç”¨å¤§æ¨¡å‹ï¼ˆæ•™å¸ˆï¼‰è®­ç»ƒå°æ¨¡å‹ï¼ˆå­¦ç”Ÿï¼‰

**æµç¨‹**ï¼š
```
å¤§æ¨¡å‹ï¼ˆæ•™å¸ˆï¼‰â†’ è½¯æ ‡ç­¾ â†’ å°æ¨¡å‹ï¼ˆå­¦ç”Ÿï¼‰
```

**ä¼˜åŠ¿**ï¼š
- ä¿æŒæ€§èƒ½
- å‡å°‘æ¨¡å‹å¤§å°
- åŠ é€Ÿæ¨ç†

**ç¤ºä¾‹**ï¼š
```python
# æ•™å¸ˆæ¨¡å‹è¾“å‡ºè½¯æ ‡ç­¾
teacher_output = teacher_model(input)
soft_labels = softmax(teacher_output / temperature)

# å­¦ç”Ÿæ¨¡å‹å­¦ä¹ è½¯æ ‡ç­¾
student_output = student_model(input)
loss = distillation_loss(student_output, soft_labels)
```

### 4. æ¨¡å‹å‹ç¼©ï¼ˆModel Compressionï¼‰

**æŠ€æœ¯**ï¼š
- ä½ç§©åˆ†è§£
- æƒé‡å…±äº«
- å‚æ•°å…±äº«

**ä¼˜åŠ¿**ï¼š
- å¤§å¹…å‡å°‘æ¨¡å‹å¤§å°
- ä¿æŒæ€§èƒ½

### 5. å›¾ä¼˜åŒ–ï¼ˆGraph Optimizationï¼‰

**æŠ€æœ¯**ï¼š
- æ“ä½œèåˆ
- å¸¸é‡æŠ˜å 
- æ­»ä»£ç æ¶ˆé™¤

**å·¥å…·**ï¼š
- TensorRT
- ONNX Runtime
- OpenVINO

**ç¤ºä¾‹**ï¼š
```python
import tensorrt as trt

# TensorRT ä¼˜åŒ–
builder = trt.Builder(logger)
network = builder.create_network()
parser = trt.OnnxParser(network, logger)
parser.parse(model_file)

# æ„å»ºä¼˜åŒ–å¼•æ“
engine = builder.build_engine(network, config)
```

## ğŸ’¡ è®­ç»ƒä¼˜åŒ–

### 1. å­¦ä¹ ç‡è°ƒåº¦

```python
# å­¦ä¹ ç‡è¡°å‡
scheduler = torch.optim.lr_scheduler.StepLR(
    optimizer, 
    step_size=30, 
    gamma=0.1
)
```

### 2. æ­£åˆ™åŒ–

```python
# L2 æ­£åˆ™åŒ–
optimizer = torch.optim.Adam(
    model.parameters(), 
    weight_decay=0.0001
)

# Dropout
model = nn.Sequential(
    nn.Linear(784, 256),
    nn.Dropout(0.5),
    nn.ReLU()
)
```

### 3. æ‰¹å½’ä¸€åŒ–

```python
# Batch Normalization
model = nn.Sequential(
    nn.Linear(784, 256),
    nn.BatchNorm1d(256),
    nn.ReLU()
)
```

## ğŸ“Š ä¼˜åŒ–ç­–ç•¥

### 1. æ¸è¿›å¼ä¼˜åŒ–

```
åŸå§‹æ¨¡å‹
    â†“
é‡åŒ–
    â†“
å‰ªæ
    â†“
è’¸é¦
    â†“
ä¼˜åŒ–æ¨¡å‹
```

### 2. è¯„ä¼°æŒ‡æ ‡

- **å‡†ç¡®ç‡**ï¼šæ¨¡å‹æ€§èƒ½
- **æ¨¡å‹å¤§å°**ï¼šå­˜å‚¨éœ€æ±‚
- **æ¨ç†æ—¶é—´**ï¼šå»¶è¿Ÿ
- **å†…å­˜ä½¿ç”¨**ï¼šèµ„æºéœ€æ±‚

### 3. æƒè¡¡è€ƒè™‘

| ä¼˜åŒ–æŠ€æœ¯ | æ€§èƒ½å½±å“ | å¤§å°å‡å°‘ | é€Ÿåº¦æå‡ |
|---------|---------|---------|---------|
| é‡åŒ– | å° | é«˜ | é«˜ |
| å‰ªæ | ä¸­ | ä¸­ | ä¸­ |
| è’¸é¦ | å° | é«˜ | é«˜ |

## ğŸ¯ å·¥å…·å’Œæ¡†æ¶

### 1. PyTorch

```python
# é‡åŒ–
torch.quantization

# å‰ªæ
torch.nn.utils.prune

# JIT ç¼–è¯‘
torch.jit.script
```

### 2. TensorFlow

```python
# TensorFlow Lite
converter = tf.lite.TFLiteConverter.from_saved_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()
```

### 3. ONNX

```python
# è½¬æ¢ä¸º ONNX
torch.onnx.export(model, input, "model.onnx")

# ONNX Runtime ä¼˜åŒ–
import onnxruntime as ort
session = ort.InferenceSession("model.onnx")
```

## âš¡ æœ€ä½³å®è·µ

### 1. å…ˆè¯„ä¼°åä¼˜åŒ–

```python
# è¯„ä¼°åŸå§‹æ¨¡å‹
baseline_accuracy = evaluate(model)
baseline_size = get_model_size(model)
baseline_speed = measure_inference_time(model)

# ä¼˜åŒ–åå¯¹æ¯”
optimized_accuracy = evaluate(optimized_model)
optimized_size = get_model_size(optimized_model)
optimized_speed = measure_inference_time(optimized_model)
```

### 2. æ¸è¿›å¼ä¼˜åŒ–

- å…ˆå°è¯•é‡åŒ–ï¼ˆå½±å“å°ï¼‰
- å†å°è¯•å‰ªæï¼ˆéœ€è¦é‡æ–°è®­ç»ƒï¼‰
- æœ€åè€ƒè™‘è’¸é¦ï¼ˆéœ€è¦æ•™å¸ˆæ¨¡å‹ï¼‰

### 3. ç¡¬ä»¶é€‚é…

- **CPU**ï¼šé‡åŒ– + å‰ªæ
- **GPU**ï¼šTensorRT ä¼˜åŒ–
- **ç§»åŠ¨è®¾å¤‡**ï¼šTensorFlow Lite
- **è¾¹ç¼˜è®¾å¤‡**ï¼šONNX Runtime

## ğŸ”— ç›¸å…³æ¦‚å¿µ

- **Model Compression**ï¼šæ¨¡å‹å‹ç¼©
- **Neural Architecture Search**ï¼šç¥ç»æ¶æ„æœç´¢
- **AutoML**ï¼šè‡ªåŠ¨æœºå™¨å­¦ä¹ 
- **Model Pruning**ï¼šæ¨¡å‹å‰ªæ
- **Quantization**ï¼šé‡åŒ–

---

*æœ€åæ›´æ–°ï¼š2024å¹´*
